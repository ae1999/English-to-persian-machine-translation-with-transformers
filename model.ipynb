{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"pytorch_x86","language":"python","name":"pytorch_x86"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"roman-minority"},"source":["import os\n","import re\n","import sys\n","import cv2\n","import tqdm\n","import time\n","import spacy \n","import math\n","import random\n","import scipy.io\n","import itertools\n","import numpy as np\n","from math import ceil\n","import pandas as pd\n","from itertools import chain\n","import matplotlib.pyplot as plt\n","from skimage.io import imread\n","from scipy.ndimage.filters import gaussian_filter\n","from sklearn.model_selection import train_test_split"],"id":"roman-minority","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"loving-nation"},"source":["from torch.nn.utils.rnn import pad_sequence \n","import torchvision.transforms as transforms\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter"],"id":"loving-nation","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"statewide-fever"},"source":["<img src=\"https://i.stack.imgur.com/eAKQu.png\">"],"id":"statewide-fever"},{"cell_type":"code","metadata":{"id":"objective-seventh"},"source":["class PositionalEmbedding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0, max_len=1000):\n","        super(PositionalEmbedding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"id":"objective-seventh","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alone-dispute"},"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, emd_size = 256, num_heads = 8, dim_key = 64, dim_value = 64):\n","        super(MultiHeadAttention, self).__init__()\n","        self.query_linear = nn.Linear(in_features=emd_size, out_features=num_heads*dim_key)\n","        self.key_linear = nn.Linear(in_features=emd_size, out_features=num_heads*dim_key)\n","        self.value_linear = nn.Linear(in_features=emd_size, out_features=num_heads*dim_value)\n","        self.resize_linear = nn.Linear(in_features=num_heads*dim_value, out_features=emd_size)\n","        self.softmax = nn.Softmax(dim=2)\n","        self.norm = nn.LayerNorm(num_heads*dim_key)\n","        self.drop = nn.Dropout(p=0.1)\n","        \n","    def forward(self, query, key, value):\n","        embedded_query = self.query_linear(query)\n","        embedded_key = self.key_linear(key)\n","        embedded_value = self.value_linear(value)\n","        dot_prod = torch.matmul(embedded_query, embedded_key.transpose(dim0 = 1, dim1 = 2))\n","        dot_prod = self.softmax(dot_prod / np.sqrt(dot_prod.shape[1]))\n","        filtered_value = torch.matmul(dot_prod, embedded_value)\n","        mha_out = self.drop(self.resize_linear(self.norm(filtered_value)))\n","        return mha_out"],"id":"alone-dispute","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cooked-advancement"},"source":["class Pos_FFN(nn.Module):\n","    \n","    def __init__(self, emd_size = 256, dim_inner = 1024):\n","        super(Pos_FFN, self).__init__()\n","        self.encode_linear = nn.Linear(in_features=emd_size, out_features=dim_inner)\n","        self.decode_linear = nn.Linear(in_features=dim_inner, out_features=emd_size)\n","        self.norm = nn.LayerNorm(emd_size)\n","        self.drop = nn.Dropout(p=0.1)\n","        \n","    def forward(self, in_features):\n","        in_features = self.encode_linear(in_features)\n","        in_features = self.decode_linear(in_features)\n","        out_features = self.drop(self.norm(in_features))\n","        return out_features"],"id":"cooked-advancement","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"digital-qualification"},"source":["class Transformer_Embedding(nn.Module):\n","    \n","    def __init__(self, vocab_size, embed_size):\n","        super(Transformer_Embedding, self).__init__()     \n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.pos_embed = PositionalEmbedding(embed_size, dropout=0)\n","        self.mha = MultiHeadAttention()\n","        self.pos_ffn = Pos_FFN()\n","    def forward(self, sentence):\n","        embeddings = self.pos_embed(self.embed(sentence))\n","        return embeddings"],"id":"digital-qualification","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"informational-providence"},"source":["class Transformer_Encoder(nn.Module):\n","    \n","    def __init__(self, emd_size = 256):\n","        super(Transformer_Encoder, self).__init__()\n","        self.mha = MultiHeadAttention()\n","        self.norm0 = nn.LayerNorm(emd_size)\n","        self.norm1 = nn.LayerNorm(emd_size)\n","        self.pos_ffn = Pos_FFN()\n","    def forward(self, embeddings):\n","        multi_head_attention_out = self.mha(embeddings, embeddings, embeddings)\n","        multi_head_attention_out = self.norm0(multi_head_attention_out)\n","        pos_ffn_out = self.pos_ffn(multi_head_attention_out)\n","        pos_ffn_out = self.norm1(pos_ffn_out + multi_head_attention_out)\n","        return pos_ffn_out"],"id":"informational-providence","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"covered-combining"},"source":["class Transformer_Decoder(nn.Module):\n","    \n","    def __init__(self, embed_size = 256, num_heads = 8, num_decoder = 3, dim_inner = 1024):\n","        super(Transformer_Decoder, self).__init__()\n","        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, dim_feedforward = dim_inner, nhead=num_heads, batch_first=True)\n","        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder)\n","    def forward(self, output_embedded, encoder_out, tgt_mask):\n","        decoder_out = self.transformer_decoder(tgt = output_embedded, memory = encoder_out, tgt_mask = tgt_mask)\n","        return decoder_out\n","    \n","    def decoder_only(self, ys, memory, tgt_mask):\n","        out = self.transformer_decoder(ys, memory, tgt_mask)\n","        return out"],"id":"covered-combining","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"controlling-olive"},"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, in_vocab_size, out_vocab_size, embed_size):\n","        super(Transformer, self).__init__()\n","        self.input_embedding = Transformer_Embedding(in_vocab_size, embed_size)\n","        self.output_embedding = Transformer_Embedding(out_vocab_size, embed_size)\n","        self.encoder0 = Transformer_Encoder()\n","        self.encoder1 = Transformer_Encoder()\n","        self.encoder2 = Transformer_Encoder()\n","\n","        self.decoder = Transformer_Decoder()\n","\n","        self.linear_output_mapping = nn.Linear(in_features=embed_size, out_features=out_vocab_size)\n","        \n","    def forward(self, in_sentences, out_sentences, tgt_mask):\n","        in_embedded = self.input_embedding(in_sentences)\n","        out_embedded = self.output_embedding(out_sentences)\n","        \n","        encoder_out = self.encoder0(in_embedded)\n","        encoder_out = self.encoder1(encoder_out)\n","        encoder_out = self.encoder2(encoder_out)\n","\n","        decoder_out = self.decoder(out_embedded, encoder_out, tgt_mask)\n","\n","        sentence_out = self.linear_output_mapping(decoder_out)\n","        sentence_out = torch.transpose(sentence_out, 1, 2)\n","        \n","        return sentence_out\n","    \n","    def encoder_out(self, in_sentences):\n","        \n","        in_embedded = self.input_embedding(in_sentences)\n","\n","        encoder_out = self.encoder0(in_embedded)\n","        encoder_out = self.encoder1(encoder_out)\n","        encoder_out = self.encoder2(encoder_out)\n","        \n","        return encoder_out\n","    \n","    def decoder_translate(self, ys, memory, tgt_mask):\n","        ys = self.output_embedding(ys)\n","        decoder_out = self.decoder.decoder_only(ys, memory, tgt_mask)\n","        sentence_out = self.linear_output_mapping(decoder_out)\n","        return sentence_out"],"id":"controlling-olive","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tribal-chicago"},"source":["Testing"],"id":"tribal-chicago"},{"cell_type":"code","metadata":{"id":"trying-clark"},"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device='cpu')) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","def translate(model, src):\n","    model.eval()\n","    start_symbol = 1\n","    num_tokens = src.shape[0]\n","\n","    src = src.to(device)\n","    memory = model.encoder_out(src)\n","    \n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n","    \n","    max_len=num_tokens + 7\n","    for i in range(max_len-1):\n","        memory = memory.to(device)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(1)).type(torch.bool)).to(device)\n","        \n","        sentence_out = model.decoder_translate(ys, memory, tgt_mask)\n","        next_word = torch.argmax(sentence_out, dim = 2)[0][-1]\n","        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","        if next_word == 2:\n","            break\n","            \n","    return ys"],"id":"trying-clark","execution_count":null,"outputs":[]}]}