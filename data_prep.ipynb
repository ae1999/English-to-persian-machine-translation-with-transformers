{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-tuesday",
   "metadata": {
    "id": "appreciated-tuesday"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import tqdm\n",
    "import time\n",
    "import spacy \n",
    "import random\n",
    "import scipy.io\n",
    "import itertools\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-vegetarian",
   "metadata": {
    "id": "beautiful-vegetarian"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-mirror",
   "metadata": {
    "id": "female-mirror"
   },
   "outputs": [],
   "source": [
    "pattern_symbol = re.compile('^[!\"#$%&\\\\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]|[!\"#$%&\\\\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]$|/')\n",
    "pattern_replace = re.compile('\\u200c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-dutch",
   "metadata": {
    "id": "raised-dutch"
   },
   "outputs": [],
   "source": [
    "# training_data_path_en = './gdrive/MyDrive/3/Data/AFEC-merged-all/AFEC-merged.en'\n",
    "# training_data_path_fa = './gdrive/MyDrive/3/Data/AFEC-merged-all/AFEC-merged.fa'\n",
    "# test_data_path = './gdrive/MyDrive/3/Data/Test/test.en'\n",
    "training_data_path_en = '../../Data/AFEC-merged-all/AFEC-merged.en'\n",
    "training_data_path_fa = '../../Data/AFEC-merged-all/AFEC-merged.fa'\n",
    "test_data_path = '../../Data/Test/test.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VTyKh6MX9o-o",
   "metadata": {
    "id": "VTyKh6MX9o-o"
   },
   "outputs": [],
   "source": [
    "import pyonmttok\n",
    "print('BPE English ...')\n",
    "eng_tokenizer = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, segment_numbers=True)\n",
    "eng_learner = pyonmttok.BPELearner(tokenizer=eng_tokenizer, symbols=22000)\n",
    "eng_learner = pyonmttok.SentencePieceLearner(vocab_size=20000, character_coverage=0.98)\n",
    "eng_learner.ingest_file(training_data_path_en)\n",
    "eng_tokenizer = eng_learner.learn(\"./BPE_ENG\")\n",
    "\n",
    "print('BPE Persian ...')\n",
    "per_tokenizer = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, segment_numbers=True)\n",
    "per_learner = pyonmttok.BPELearner(tokenizer=per_tokenizer, symbols=18000)\n",
    "per_learner = pyonmttok.SentencePieceLearner(vocab_size=15000, character_coverage=0.98)\n",
    "per_learner.ingest_file(training_data_path_fa)\n",
    "per_tokenizer = per_learner.learn(\"./BPE_PER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-decline",
   "metadata": {
    "id": "spanish-decline"
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences, min_word_freq = 7):\n",
    "        vocabulary = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        rev_vocabulary = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        min_word_frequency = min_word_freq\n",
    "        \n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        counter, count = 1, len(sentences)\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == min_word_frequency:\n",
    "                    rev_vocabulary[word] = idx\n",
    "                    vocabulary[idx] = word\n",
    "                    idx += 1\n",
    "        counter += 1\n",
    "        print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "        print('---done!---')\n",
    "        return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-mount",
   "metadata": {
    "id": "intermediate-mount"
   },
   "outputs": [],
   "source": [
    "def numericalize_sentence(text, rev_vocabulary):\n",
    "    numericalized_caption = [rev_vocabulary[\"<SOS>\"]]\n",
    "    numericalized_caption += [\n",
    "        rev_vocabulary[token] if token in rev_vocabulary else rev_vocabulary[\"<UNK>\"]\n",
    "        for token in text\n",
    "    ]\n",
    "    numericalized_caption.append(rev_vocabulary[\"<EOS>\"])\n",
    "    return numericalized_caption\n",
    "\n",
    "def reverse_numericalize(sentence, vocab):\n",
    "\n",
    "        strings = []\n",
    "            \n",
    "        for token in sentence:\n",
    "            token = int(token.item())\n",
    "            if token in vocab:\n",
    "                strings.append(vocab[token])\n",
    "            else:\n",
    "                strings.append(vocab[3])\n",
    "            if strings[-1] == '<EOS>':\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if '<SOS>' in strings:\n",
    "            strings.remove('<SOS>')\n",
    "        if '<EOS>' in strings:\n",
    "            strings.remove('<EOS>')\n",
    "        while '<PAD>' in strings:\n",
    "            strings.remove('<PAD>')\n",
    "            \n",
    "        # sentence = ''\n",
    "        # for i in strings:\n",
    "        #     sentence += i\n",
    "        #     sentence += ' '\n",
    "            \n",
    "        return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-advance",
   "metadata": {
    "id": "convertible-advance"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, eng, per):\n",
    "        self.X, self.y = eng, per\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        per = self.y[index]\n",
    "        eng = self.X[index]\n",
    "        return torch.tensor(eng), torch.tensor(per)\n",
    "\n",
    "def PadSequence(batch):\n",
    "    eng_sequences = [x[0] for x in batch]\n",
    "    per_sequences = [x[1] for x in batch]\n",
    "    eng_sequences_padded = torch.nn.utils.rnn.pad_sequence(eng_sequences, batch_first=True)\n",
    "    per_sequences_padded = torch.nn.utils.rnn.pad_sequence(per_sequences, batch_first=True)\n",
    "    lengths = torch.LongTensor([len(x) for x in eng_sequences])\n",
    "    return eng_sequences_padded, per_sequences_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7uavCLKP6CWe",
   "metadata": {
    "id": "7uavCLKP6CWe"
   },
   "outputs": [],
   "source": [
    "def get_test_data(dp):\n",
    "    english = []\n",
    "    f0 = open(test_data_path)\n",
    "    for eng in f0:\n",
    "        english.append(eng_tokenizer.tokenize(eng)[0])\n",
    "\n",
    "    num_english = []\n",
    "\n",
    "    for i in english:\n",
    "        num_english.append(numericalize_sentence(i, dp.eng_rev_vocab))\n",
    "    \n",
    "    return num_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-integer",
   "metadata": {
    "id": "controlling-integer"
   },
   "outputs": [],
   "source": [
    "class Data_prep():\n",
    "    def __init__(self):\n",
    "        english, persian = [], []\n",
    "        f0 = open(training_data_path_en)\n",
    "        f1 = open(training_data_path_fa)\n",
    "        for eng, per in zip(f0, f1):\n",
    "            english.append(eng_tokenizer.tokenize(eng)[0])\n",
    "            persian.append(per_tokenizer.tokenize(per)[0])\n",
    "        print('creating vocabulary.')\n",
    "        self.eng_vocab, self.eng_rev_vocab = create_vocabulary(english, min_word_freq = 8)\n",
    "        self.per_vocab, self.per_rev_vocab = create_vocabulary(persian, min_word_freq = 6)\n",
    "\n",
    "        self.num_english, self.num_persian = [], []\n",
    "\n",
    "        for i in english:\n",
    "            self.num_english.append(numericalize_sentence(i, self.eng_rev_vocab))\n",
    "        for i in persian:\n",
    "            self.num_persian.append(numericalize_sentence(i, self.per_rev_vocab))\n",
    "            \n",
    "    def get_data(self):\n",
    "        return train_test_split(self.num_english, self.num_persian, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test_data(dp):\n",
    "    per_path0 = '../../Data/Test/test.fa0'\n",
    "    per_path1 = '../../Data/Test/test.fa1'\n",
    "    per_path2 = '../../Data/Test/test.fa2'\n",
    "    per_path3 = '../../Data/Test/test.fa3'\n",
    "    eng_path = '../../Data/Test/test.en'\n",
    "    persian = []\n",
    "    english = []\n",
    "    f0 = open(per_path0)\n",
    "    f1 = open(per_path1)\n",
    "    f2 = open(per_path2)\n",
    "    f3 = open(per_path3)\n",
    "    e0 = open(eng_path)\n",
    "    for p0, p1, p2, p3, eng in zip(f0, f1, f2, f3, e0):\n",
    "        while pattern_symbol.search(eng):\n",
    "                eng = pattern_symbol.sub('', eng)\n",
    "                \n",
    "        while pattern_symbol.search(p0):\n",
    "            p0 = pattern_symbol.sub('', p0)\n",
    "        while pattern_replace.search(p0):\n",
    "            p0 = pattern_replace.sub(' ', p0)\n",
    "            \n",
    "        while pattern_symbol.search(p1):\n",
    "            p1 = pattern_symbol.sub('', p1)\n",
    "        while pattern_replace.search(p1):\n",
    "            p1 = pattern_replace.sub(' ', p1)\n",
    "            \n",
    "        while pattern_symbol.search(p2):\n",
    "            p2 = pattern_symbol.sub('', p2)\n",
    "        while pattern_replace.search(p2):\n",
    "            p2 = pattern_replace.sub(' ', p2)\n",
    "        english.append(eng.split())\n",
    "        persian.append([p0.split(), p1.split(), p2.split(), p3.split()])\n",
    "        \n",
    "    num_english = []\n",
    "\n",
    "    for i in english:\n",
    "        num_english.append(numericalize_sentence(i, dp.eng_rev_vocab))\n",
    "    \n",
    "    return num_english, persian"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_prep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
